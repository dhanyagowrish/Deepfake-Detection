{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to test the trained model to check if a video is a deepfake or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from facenet_pytorch import MTCNN\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1139,
     "status": "ok",
     "timestamp": 1592452951062,
     "user": {
      "displayName": "Dhanya Gowrish",
      "photoUrl": "",
      "userId": "05916578285199399612"
     },
     "user_tz": -330
    },
    "id": "8nrHGeLm66PF"
   },
   "outputs": [],
   "source": [
    "# function to load the ResNet-50 model with the trained weights in checkpoint file\n",
    "\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    checkpoint=torch.load(checkpoint_path)\n",
    "    model_conv=torchvision.models.resnet50(pretrained=True)\n",
    "    \n",
    "    for param in model_conv.parameters():\n",
    "        param.requires_grad=False\n",
    "        \n",
    "    num_ftrs=model_conv.fc.in_features\n",
    "    model_conv.fc=nn.Linear(num_ftrs,2)\n",
    "    \n",
    "    #loading the weights\n",
    "    model_conv.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    model_conv.eval()\n",
    "    return model_conv\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1113,
     "status": "ok",
     "timestamp": 1592452952980,
     "user": {
      "displayName": "Dhanya Gowrish",
      "photoUrl": "",
      "userId": "05916578285199399612"
     },
     "user_tz": -330
    },
    "id": "gbAzRlPX7CBB"
   },
   "outputs": [],
   "source": [
    "#takes PIL image as input. Outputs a tensor. Normalization is done (same as what's done for validation images)\n",
    "\n",
    "def applyTransforms(inp):\n",
    "    outp = transforms.functional.resize(inp, [224,224])\n",
    "    outp = transforms.functional.to_tensor(outp)\n",
    "    outp = transforms.functional.normalize(outp, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    return outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1249,
     "status": "ok",
     "timestamp": 1592452957285,
     "user": {
      "displayName": "Dhanya Gowrish",
      "photoUrl": "",
      "userId": "05916578285199399612"
     },
     "user_tz": -330
    },
    "id": "iLjLojI87Gb1"
   },
   "outputs": [],
   "source": [
    "# function that :\n",
    "# splits passed video into frames and identifies faces within the frames\n",
    "# does forward propogation for all these face frames through the model loaded with trained weights \n",
    "# each forward propogation results in a probability for the frame being \"fake\" or \"real\" \n",
    "# function returns the average probability and percentage of all the frames being real and fake \n",
    "# class with highest probability is considered the detected label of the video\n",
    "\n",
    "def myVideo(file_name,model):\n",
    "    mtcnn = MTCNN(margin=20, keep_all=True, post_process=False, device=device)\n",
    "    # Load video\n",
    "    v_cap = cv2.VideoCapture(file_name)\n",
    "    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Loop through video, taking some no of frames to form a batch   (here, every 30th frame)\n",
    "    frames = []\n",
    "    for i in tqdm(range(v_len)):\n",
    "    \n",
    "        # Load frame\n",
    "        success = v_cap.grab()\n",
    "        if i % 30 == 0:\n",
    "            success, frame = v_cap.retrieve()\n",
    "        else:\n",
    "            continue\n",
    "        if not success:\n",
    "            continue\n",
    "        \n",
    "        # Add to batch\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(Image.fromarray(frame))\n",
    "\n",
    "\n",
    "\n",
    "    #detect faces in frames &  saving frames to file\n",
    "    f=\"test\\test_frames\" + \"\\\\\"\n",
    "    frames_paths = [f+'image' + str(i) +'.jpg' for i in range(len(frames))]\n",
    "    faces = mtcnn(frames,save_path=frames_paths)\n",
    "    \n",
    "    path=os.listdir(\"test\\test_frames\")\n",
    "    vals_f=[]\n",
    "    vals_r=[]\n",
    "    for x in path:\n",
    "        img = Image.open(\"test\\test_frames\" + \"\\\\\" + x)\n",
    "        imageTensor = applyTransforms(img)\n",
    "        minibatch = torch.stack([imageTensor])\n",
    "        #model_conv(minibatch)\n",
    "        softMax = nn.Softmax(dim = 1)\n",
    "        preds = softMax(model(minibatch))\n",
    "        vals_f.append(preds[0,0].item())\n",
    "        vals_r.append(preds[0,1].item())\n",
    "    \n",
    "    av=sum(vals_f)/len(path)\n",
    "    print(\"average probability of fakeness:\",av)\n",
    "    print('Percentage of fakeness: {:.4f}'.format(av*100))\n",
    "    \n",
    "    av=sum(vals_r)/len(path)\n",
    "    print(\"average probability of realness:\",av)\n",
    "    print('Percentage of realness: {:.4f}'.format(av*100))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1206,
     "status": "ok",
     "timestamp": 1592454643556,
     "user": {
      "displayName": "Dhanya Gowrish",
      "photoUrl": "",
      "userId": "05916578285199399612"
     },
     "user_tz": -330
    },
    "id": "7NGTQDx67Iky"
   },
   "outputs": [],
   "source": [
    "# call this function to test a video \n",
    "\n",
    "def testing(name,model):\n",
    "    \n",
    "    #import os\n",
    "    f=\"test\\test_frames\"\n",
    "    reqd=os.listdir(f)\n",
    "    \n",
    "    if len(reqd)!=0:\n",
    "        for i in reqd:\n",
    "            os.remove(f+\"\\\\\"+i)\n",
    "    \n",
    "    path=\"\\user\\test\\test_videos\" + \"\\\\\" + name\n",
    "    myVideo(path,model)\n",
    "\n",
    "    import json\n",
    "    fp=open(r\"\\user\\test\\metadata.json\",)\n",
    "    data=json.load(fp)\n",
    "    path=name\n",
    "    #print('The true label is:',data[path]['label'])\n",
    "    fp.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Call load_checkpoint() function ( to load the trained weights into the ResNet-50 model )\n",
    "model_conv=load_checkpoint(\"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6920,
     "status": "ok",
     "timestamp": 1592417963789,
     "user": {
      "displayName": "Dhanya Gowrish",
      "photoUrl": "",
      "userId": "05916578285199399612"
     },
     "user_tz": -330
    },
    "id": "OEJOWB1EgTI6",
    "outputId": "4868cbac-972d-4ac0-f277-73023887f3d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:01<00:00, 162.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average probability of fakeness: 0.7003347824017206\n",
      "Percentage of fakeness: 70.0335\n",
      "average probability of realness: 0.29966522303099435\n",
      "Percentage of realness: 29.9665\n",
      "The true label is: FAKE\n"
     ]
    }
   ],
   "source": [
    "#example of testing a video \"funny_deepfake.mp4\" \n",
    "\n",
    "testing('funny_deepfake.mp4',model_conv)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "testing_file.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05395d4cdda1447b8cd9293f9ab1aa47": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_238367460b334370a513a619e6daeaa6",
       "IPY_MODEL_dc0fb44ea10f44cb9fbc47fadf6a6caf"
      ],
      "layout": "IPY_MODEL_829e1f9c76694b92b603967558277dec"
     }
    },
    "120b72d33ed6491da9f6e0b2dcea4534": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d01d959ea74478fb7fa7c8ada7be855": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "238367460b334370a513a619e6daeaa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f263e5bd43b4617a567c832edb36935",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd865162ebf84baeba63f41b110fbf13",
      "value": 102502400
     }
    },
    "4f263e5bd43b4617a567c832edb36935": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "829e1f9c76694b92b603967558277dec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd865162ebf84baeba63f41b110fbf13": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "dc0fb44ea10f44cb9fbc47fadf6a6caf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_120b72d33ed6491da9f6e0b2dcea4534",
      "placeholder": "​",
      "style": "IPY_MODEL_1d01d959ea74478fb7fa7c8ada7be855",
      "value": " 97.8M/97.8M [24:28&lt;00:00, 69.8kB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
